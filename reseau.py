# -*- coding: utf-8 -*-
"""Réseau.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y0WZ-8AcptnzW_-xnxJmzD_0WlonHFNw
"""

import os
import pandas as pd
import numpy as np
import random as rd

from keras.models import Model
from keras.layers import CuDNNGRU, CuDNNLSTM, Input, BatchNormalization, Flatten, Lambda, Concatenate
import numpy as np # linear algebra
from numpy import newaxis
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM, GRU
from keras.models import Sequential
from keras import optimizers
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from keras.models import Sequential

from google.colab import drive
drive.mount('/content/drive')

from sklearn.model_selection import train_test_split

files = [pd.read_csv('/content/drive/My Drive/Colab Notebooks/'+el+'') for el in os.listdir('/content/drive/My Drive/Colab Notebooks') if el[:6] == 'attack']
train_set = []
test_set = []

for file in files:
  a, b = train_test_split(file, test_size=0.2)
  
  train_set.append(a)
  test_set.append(b)

train_set = pd.concat(train_set)
test_set = pd.concat(test_set)

train_set = train_set.dropna().sort_values(['Identifiant du destinataire', 'BSM'])
test_set = test_set.dropna().sort_values(['Identifiant du destinataire', 'BSM'])

from keras.utils import to_categorical

test1 = np.copy(train_set['Label'].values)
test2 = np.copy(test_set['Label'].values)

# Enculé de Python
test1[test1 == 4] = 3
test1[test1 == 8] = 4
test1[test1 == 16] = 5

test2[test2 == 4] = 3
test2[test2 == 8] = 4
test2[test2 == 16] = 5

y_train = to_categorical(test1)
y_test = to_categorical(test2)

def build_chunk(i, N, Set, labels):
    id_test = Set['Identifiant du destinataire'].iloc[i]
    
    while i < len(Set)-1 and Set.iloc[i,-1] != 0 and N > 1:
        i += 1
    
    cmpt = 0
    chunkX = []
    chunkY = []
    
    while i+cmpt < len(Set)-1 and Set['Identifiant du destinataire'].iloc[i+cmpt] == id_test and len(chunkX) < N-1 :
        if Set.iloc[i+cmpt,-1] != 0 :
            cmpt += 1
        else:
            chunkX.append(Set.iloc[i+cmpt, :-1].values)
            cmpt += 1
    
    chunkX.append(Set.iloc[i+cmpt, :-1].values)
    chunkY = labels[i+cmpt] #chunkY.append(Set.iloc[i+cmpt+1, -1])
    
    return np.array(chunkX), np.array(chunkY)

def batchs(batch_size, N, Set, labels):
    batchX = []
    batchY = []
    
    for i in range(batch_size):
        a = rd.randint(0, Set.shape[0]-N)
        
        chunkX, chunkY = build_chunk(a, N, Set, labels)
        
        if len(chunkX) == N :
            batchX.append(chunkX)
            batchY.append(chunkY)
    
    return np.array(batchX), np.array(batchY)

def get_batch(batch_size, N, Set, labels):
    while True:
        yield batchs(batch_size, N, Set, labels)

model = Sequential()
model.add(CuDNNLSTM(300, input_shape=(N,17)))
#model.add(Dropout(0.2))

#model.add(Flatten())
model.add(BatchNormalization())

model.add(Dense(200))
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(Dense(64))
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(Dense(6, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

from keras.optimizers import Adam

# First part
input1 = Input(shape=(N-1,17))
x1 = CuDNNLSTM(1000)(input1)
#x1 = Flatten()(x1)
x1 = BatchNormalization(momentum=0.8)(x1)

model1 = Model(input1, x1)

# Second part
input = Input(shape=(N,17))

input1 = Lambda(lambda x: x[:,:-1,:])(input)
input2 = Lambda(lambda x: x[:,-1,:])(input)

x = Concatenate()([input2, model1(input1)])

x = Dropout(0.5)(x)

x = Dense(600)(x)
x = Activation('relu')(x)
x = BatchNormalization(momentum=0.8)(x)

x = Dense(300)(x)
x = Activation('relu')(x)
x = BatchNormalization(momentum=0.8)(x)

x = Dense(100)(x)
x = Activation('relu')(x)
x = BatchNormalization(momentum=0.8)(x)

x = Dense(6, activation="softmax")(x)

model = Model(input, x)
model.compile(loss="categorical_crossentropy", optimizer=Adam(lr=1e-2), metrics=["accuracy"])

model.summary()

N = 5
batch_size = 32

model.fit_generator(
    get_batch(batch_size, N, train_set, y_train),
    steps_per_epoch = int(len(train_set) / (N * batch_size)),
    epochs=10, 
    validation_data = get_batch(batch_size, N, test_set, y_test),
    validation_steps = int(len(test_set) / (N * batch_size)),
    use_multiprocessing=True,
    workers=-1
)

model = Sequential()
#model.add(CuDNNLSTM(300, input_shape=(N,17)))
#model.add(Dropout(0.2))

#model.add(Flatten())
#model.add(BatchNormalization())

model.add(Flatten(input_shape=(N,17)))
model.add(Dense(500))
model.add(Activation('relu'))
model.add(BatchNormalization(momentum=0.8))

model.add(Dense(200))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Dense(64))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Dense(6, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

input1 = Input(shape=(N-1, 17))

x1 = Flatten()(input1)
x1 = BatchNormalization(momentum=0.8)(x1)

x1 = Dense(N*17)(x1)
x1 = Activation('relu')(x1)
x1 = BatchNormalization(momentum=0.8)(x1)

x1 = Dropout(0.1)(x1)

x1 = Dense(4*N)(x1)
x1 = Activation('relu')(x1)
x1 = BatchNormalization(momentum=0.8)(x1)

model1 = Model(input1, x1)

input = Input(shape=(N,17))

input1 = Lambda(lambda x: x[:,:-1,:])(input)
input2 = Lambda(lambda x: x[:,-1,:])(input)

x = BatchNormalization(momentum=0.8)(input2)

y = model1(input1)
x = Concatenate()([x, y])

x = Dense(40)(x)
x = Activation('relu')(x)
x = BatchNormalization(momentum=0.8)(x)

x = Dropout(0.2)(x)

x = Dense(20)(x)
x = Activation('relu')(x)
x = BatchNormalization(momentum=0.8)(x)

x = Dropout(0.1)(x)

x = Dense(6, activation='softmax')(x)

model = Model(input, x)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()